---
layout:     post
title:      Question
subtitle:   Question
date:       2019-09-26
author:     lulongji
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - question
---

# 说明
这么多年的研发生涯中遇到过很多问题、但是几乎都没有记录下来、所以从今天开始、把遇到的问题及处理的方式做个总结。

# 问题及解决办法

- 问题：`脚本启动无日志无反应`

    - 排查原因：发现脚本开启debug模式、启动的时候debug 端口被占用导致、无法启动且日志被屏蔽！


            APP_DEBUGE="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=18894"

        
    - 解决问题：把之前启动屏蔽日志模式打开、并增加是否开始执行脚本并打印日志方式！
    
- 问题：`websocket有部分浏览器不兼容问题`

    - 问题分析：目前用户侧和坐席侧双方都为websocket服务、但用户侧浏览器不固定需要处理兼容性问题、所以增加cometserver的http方式向下兼容、保证移动端和pc端浏览器在不支持webscoket的情况下可以处理相应会话。
    - 解决问题：增加cometserver服务、利用DeferredResult属性充分利用http资源做http长轮询处理！

- 问题：`近期websocket 连接总会在1分钟之后默认断开连接、发起重连机制（很频繁）`

    - 排查问题：发现在ngixn做处理的时候超时时间太短、导致websocket在1分钟超时自动断开、所以websocket的重连机制才会频繁发起

    - 解决方案：增加nginx的超时时间 `proxy_read_timeout `

        location /websocket {
            proxy_pass http://websocket;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection $connection_upgrade;
            proxy_read_timeout          600s;
        }



- 问题：`最近遇到一个奇怪的问题、相同的springboot服务部署到不同的虚拟机服务器上初始化占用内存居然差距很大、比如第一台程序占了1.7g、但是第二台机器占用200M左右？`

    - 排查问题：发现服务链接数、资源数都相同、并且用jvm提供的工具定位（根本没有办法定位），所以使用linux提供的内存映射的命令pmap进行分析查看原因。

    pmap -d  3121

    打印如下：

            ▽
        00002aece8000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aece8021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aecec000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aecec021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aecf0000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aecf0021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aecf4000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aecf4021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aecf8000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aecf8021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aecfc000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aecfc021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed00000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed00021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed04000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed04021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed08000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed08021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed0c000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed0c021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed10000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed10021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed14000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed14021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed18000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed18021000   65404 ----- 0000000000000000 000:00000   [ anon ]

        ▽
        00002aed1c000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed1c021000   65404 ----- 0000000000000000 000:00000   [ anon ]
        00002aed20000000     132 rw--- 0000000000000000 000:00000   [ anon ]
        00002aed20021000   65404 ----- 0000000000000000 000:00000   [ anon ]

        ▽
        00007fff3c111000     132 rw--- 0000000000000000 000:00000   [ stack ]
        00007fff3c1f2000       8 r-x-- 0000000000000000 000:00000   [ anon ]
        ffffffffff600000       4 r-x-- 0000000000000000 000:00000   [ anon ]
        mapped: 11686600K    writeable/private: 2387964K    shared: 2760K


    - 请注意65404这一行，种种迹象表明，这个再加上它上面那一行（在这里是132）就是增加的那个64M）。64M左右的内存块有大概30个，约占1920M （1.8G）。

    Google或者必应 查找这块内存占用为什么这么规整、根源就是glibc的malloc问题，想知道Linux下glibc的内存管理机制、自行查文档。

    简而言之，就是glibc分配内存的时候，大内存从从中央分配区分配，小内存从线程创建时，预先分配的缓存区分配。glibc为了分配内存的性能的问题，使用了很多叫做arena的memory pool,缺省配置在64bit下面是每一个arena为64M，一个进程可以最多有 cores * 8个arena。假设你的机器是4核的，那么最多可以有4 * 8 = 32个arena，也就是使用32 * 64 = 2048M内存。 当然你也可以通过设置环境变量来改变arena的数量.

    例如export MALLOC_ARENA_MAX=1，如果不考虑内存分配的性能，遇到这样的问题时，可使用export MALLOC_ARENA_MAX=1禁用per thread arena，只用main arena，多个线程共用一个arena内存池。如果考虑到性能，可使用tcmalloc或jemalloc替代操作系统自带的glibc管理内存。

    
    - 解决方案: 所以，解决办法就是：export MALLOC_ARENA_MAX=1


- 问题：`w`

    - 排查问题：

    - 解决方案：



# 声明
本文只做学习参考，如有任何问题的地方欢迎指正。

我的邮箱：
- ```lulongji2011@163.com```